from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
from tools.base import Tool, ToolInput, ToolOutput
from tools.spec_tool import FunctionSpec
from core import CodeExecutor, ExecutionStatus
from cognitive.line_effectiveness_validator import LineEffectivenessValidator


class ValidateInput(ToolInput):
    """éªŒè¯ä»£ç çš„è¾“å…¥"""
    code: str = Field(description="è¦éªŒè¯çš„ä»£ç ", min_length=1)
    spec: FunctionSpec = Field(description="å‡½æ•°è§„èŒƒ")


class TestResult(BaseModel):
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    test_name: str = Field(description="æµ‹è¯•åç§°")
    passed: bool = Field(description="æ˜¯å¦é€šè¿‡")
    input_values: Dict[str, Any] = Field(description="è¾“å…¥å€¼")
    expected_output: Any = Field(description="æœŸæœ›è¾“å‡º")
    actual_output: Optional[Any] = Field(default=None, description="å®é™…è¾“å‡º")
    error: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

    class Config:
        """Pydanticé…ç½®"""
        extra = "forbid"


class ValidationResult(BaseModel):
    """éªŒè¯ç»“æœ"""
    is_valid: bool = Field(description="æ˜¯å¦é€šè¿‡æ‰€æœ‰æµ‹è¯•")
    total_tests: int = Field(description="æ€»æµ‹è¯•æ•°")
    passed_count: int = Field(description="é€šè¿‡çš„æµ‹è¯•æ•°")
    test_results: List[TestResult] = Field(description="è¯¦ç»†æµ‹è¯•ç»“æœ")
    suggestions: List[str] = Field(default_factory=list, description="æ”¹è¿›å»ºè®®")

    # æ–°å¢ï¼šè¡Œæœ‰æ•ˆæ€§æ£€æŸ¥ç»“æœ
    line_effectiveness_score: Optional[float] = Field(default=None, description="ä»£ç è¡Œæœ‰æ•ˆæ€§è¯„åˆ† (0-1)")
    line_effectiveness_analysis: Optional[Dict[str, Any]] = Field(default=None, description="è¡Œæœ‰æ•ˆæ€§åˆ†æè¯¦æƒ…")
    has_redundant_code: bool = Field(default=False, description="æ˜¯å¦å­˜åœ¨å†—ä½™ä»£ç ")

    class Config:
        """Pydanticé…ç½®"""
        extra = "forbid"


class ValidateTool(Tool):
    """ä»£ç éªŒè¯å·¥å…·

    æ‰§è¡Œä»£ç å¹¶æ ¹æ®è§„èŒƒéªŒè¯å…¶æ­£ç¡®æ€§ï¼Œè¿”å›è¯¦ç»†çš„æµ‹è¯•ç»“æœã€‚
    """

    name = "validate_code"
    description = "æ‰§è¡Œä»£ç å¹¶æ ¹æ®è§„èŒƒéªŒè¯å…¶æ­£ç¡®æ€§ï¼Œè¿”å›è¯¦ç»†çš„æµ‹è¯•ç»“æœ"
    input_schema = ValidateInput

    def __init__(self):
        super().__init__()
        self.executor = CodeExecutor(timeout=30.0, enable_security=True)
        self.line_validator = LineEffectivenessValidator()

    def _execute_impl(self, input_data: ValidateInput) -> ToolOutput:
        """éªŒè¯ä»£ç """
        spec = input_data.spec
        code = input_data.code

        if not spec.examples:
            return ToolOutput.warning_result(
                data=ValidationResult(
                    is_valid=True,
                    total_tests=0,
                    passed_count=0,
                    test_results=[],
                    suggestions=["è§„èŒƒä¸­æ²¡æœ‰æµ‹è¯•ç”¨ä¾‹ï¼Œæ— æ³•éªŒè¯ä»£ç æ­£ç¡®æ€§"]
                ),
                message="æ²¡æœ‰æµ‹è¯•ç”¨ä¾‹å¯ä¾›éªŒè¯",
                code_length=len(code),
                function_name=spec.name
            )

        test_results = []

        # åŸºäºè§„èŒƒä¸­çš„examplesç”Ÿæˆæµ‹è¯•
        for idx, example in enumerate(spec.examples):
            result = self._run_single_test(
                code=code,
                func_name=spec.name,
                test_name=f"Example_{idx+1}",
                inputs=example.inputs,
                expected_output=example.expected_output
            )
            test_results.append(result)

        # ç»Ÿè®¡ç»“æœ
        passed_count = sum(1 for r in test_results if r.passed)
        total_tests = len(test_results)
        is_valid = passed_count == total_tests

        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        suggestions = self._generate_suggestions(test_results, spec)

        # ã€æ–°å¢ã€‘æ‰§è¡Œè¡Œæœ‰æ•ˆæ€§æ£€æŸ¥
        line_effectiveness_report = self.line_validator.analyze_code(code, spec.purpose)
        line_effectiveness_suggestions = self.line_validator.suggest_optimizations(line_effectiveness_report)

        validation_result = ValidationResult(
            is_valid=is_valid,
            total_tests=total_tests,
            passed_count=passed_count,
            test_results=test_results,
            suggestions=suggestions,
            # ã€æ–°å¢ã€‘è¡Œæœ‰æ•ˆæ€§æ£€æŸ¥ç»“æœ
            line_effectiveness_score=line_effectiveness_report.effectiveness_score,
            line_effectiveness_analysis={
                "total_lines": line_effectiveness_report.total_lines,
                "essential_lines": line_effectiveness_report.essential_lines,
                "important_lines": line_effectiveness_report.important_lines,
                "optional_lines": line_effectiveness_report.optional_lines,
                "redundant_lines": line_effectiveness_report.redundant_lines,
                "unused_lines": line_effectiveness_report.unused_lines,
            },
            has_redundant_code=(line_effectiveness_report.redundant_lines > 0 or
                               line_effectiveness_report.unused_lines > 0)
        )

        # æ·»åŠ è¡Œæœ‰æ•ˆæ€§å»ºè®®åˆ°å»ºè®®åˆ—è¡¨
        if line_effectiveness_suggestions:
            suggestions.append("\nğŸ“Š è¡Œæœ‰æ•ˆæ€§ä¼˜åŒ–å»ºè®®:")
            suggestions.extend(line_effectiveness_suggestions)

        if is_valid:
            return ToolOutput.success_result(
                data=validation_result,
                message=f"æ‰€æœ‰æµ‹è¯•é€šè¿‡: {passed_count}/{total_tests}ï¼Œè¡Œæœ‰æ•ˆæ€§è¯„åˆ†: {line_effectiveness_report.effectiveness_score:.2f}/1.0",
                function_name=spec.name,
                test_count=total_tests,
                effectiveness_score=line_effectiveness_report.effectiveness_score
            )
        else:
            return ToolOutput.warning_result(
                data=validation_result,
                message=f"éƒ¨åˆ†æµ‹è¯•å¤±è´¥: {passed_count}/{total_tests}",
                function_name=spec.name,
                failed_tests=total_tests - passed_count
            )

    def _run_single_test(
        self,
        code: str,
        func_name: str,
        test_name: str,
        inputs: Dict[str, Any],
        expected_output: Any
    ) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        # æ„å»ºæµ‹è¯•ä»£ç 
        try:
            args_str = ", ".join([f"{k}={repr(v)}" for k, v in inputs.items()])
            test_code = f"""
{code}

# è¿è¡Œæµ‹è¯•
try:
    result = {func_name}({args_str})
    print("RESULT:", repr(result))
except Exception as e:
    print("ERROR:", str(e))
    print("ERROR_TYPE:", type(e).__name__)
"""

            exec_result = self.executor.run(test_code)

            if exec_result.status == ExecutionStatus.SUCCESS:
                # è§£æè¾“å‡º
                output = exec_result.stdout.strip()

                if output.startswith("RESULT:"):
                    actual_output_str = output.replace("RESULT:", "").strip()
                    try:
                        actual_output = eval(actual_output_str)
                        passed = actual_output == expected_output

                        return TestResult(
                            test_name=test_name,
                            passed=passed,
                            input_values=inputs,
                            expected_output=expected_output,
                            actual_output=actual_output,
                            error=None if passed else f"æœŸæœ› {expected_output}, å®é™… {actual_output}"
                        )
                    except Exception as e:
                        return TestResult(
                            test_name=test_name,
                            passed=False,
                            input_values=inputs,
                            expected_output=expected_output,
                            error=f"æ— æ³•è§£æè¾“å‡º: {actual_output_str}"
                        )
                elif "ERROR:" in output:
                    error_lines = output.split('\n')
                    error_msg = ""
                    for line in error_lines:
                        if line.startswith("ERROR:"):
                            error_msg = line.replace("ERROR:", "").strip()
                            break

                    return TestResult(
                        test_name=test_name,
                        passed=False,
                        input_values=inputs,
                        expected_output=expected_output,
                        error=f"è¿è¡Œæ—¶å¼‚å¸¸: {error_msg}"
                    )
                else:
                    return TestResult(
                        test_name=test_name,
                        passed=False,
                        input_values=inputs,
                        expected_output=expected_output,
                        error="æ— æ³•è§£æå‡½æ•°è¾“å‡º"
                    )
            else:
                return TestResult(
                    test_name=test_name,
                    passed=False,
                    input_values=inputs,
                    expected_output=expected_output,
                    error=exec_result.error or "ä»£ç æ‰§è¡Œå¤±è´¥"
                )

        except Exception as e:
            return TestResult(
                test_name=test_name,
                passed=False,
                input_values=inputs,
                expected_output=expected_output,
                error=f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            )

    def _generate_suggestions(
        self,
        test_results: List[TestResult],
        spec: FunctionSpec
    ) -> List[str]:
        """æ ¹æ®æµ‹è¯•ç»“æœç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        failed_tests = [r for r in test_results if not r.passed]

        if not failed_tests:
            return ["[SUCCESS] All tests passed, code meets requirements!"]

        # åˆ†æå¤±è´¥åŸå› 
        error_types = {}
        for result in failed_tests:
            if result.error:
                if "æœŸæœ›" in result.error and "å®é™…" in result.error:
                    error_types["output_mismatch"] = error_types.get("output_mismatch", 0) + 1
                elif "å¼‚å¸¸" in result.error or "Exception" in result.error:
                    error_types["runtime_error"] = error_types.get("runtime_error", 0) + 1
                else:
                    error_types["other"] = error_types.get("other", 0) + 1

        # ç”Ÿæˆå…·ä½“å»ºè®®
        if error_types.get("output_mismatch", 0) > 0:
            suggestions.append("[CHECK] Review function logic to ensure return values match expectations")

        if error_types.get("runtime_error", 0) > 0:
            suggestions.append("[HANDLE] Handle runtime exceptions and check edge cases")

        # æ£€æŸ¥æ˜¯å¦å¤„ç†äº†æ‰€æœ‰è¾¹ç•Œæƒ…å†µ
        if spec.edge_cases:
            suggestions.append(f"[EDGE] Ensure handling of these edge cases: {', '.join(spec.edge_cases[:2])}")

        return suggestions
